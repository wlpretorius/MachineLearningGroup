{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection (Best Subset Selection, Stagewise Regression, AIC, BIC) & Regularization (Lasso & Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/regulate-your-regression-model-with-ridge-lasso-and-elasticnet-92735e192e34\n",
    "\n",
    "https://towardsdatascience.com/how-to-do-cross-validation-effectively-1bbeb1d69ee8\n",
    "\n",
    "https://towardsdatascience.com/two-common-pitfalls-to-avoid-when-doing-cross-validation-c68ed79c0e4e\n",
    "\n",
    "https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df\n",
    "\n",
    "https://mlu-explain.github.io/cross-validation/\n",
    "\n",
    "https://mlu-explain.github.io/bias-variance/\n",
    "\n",
    "https://towardsdatascience.com/simple-stepwise-and-weighted-regression-model-53a31d9e4746\n",
    "\n",
    "https://towardsdatascience.com/visualizing-sklearn-cross-validation-k-fold-shuffle-split-and-time-series-split-a13221eb5a56\n",
    "\n",
    "https://towardsdatascience.com/python-libraries-for-interpretable-machine-learning-c476a08ed2c7\n",
    "\n",
    "https://towardsdatascience.com/explain-machine-learning-models-using-shap-library-e05a1583c34f\n",
    "\n",
    "https://towardsdatascience.com/crafting-one-pipeline-for-machine-learning-steps-373f03e44e1b\n",
    "\n",
    "https://medium.com/@ali.soleymani.co/stop-using-grid-search-or-random-search-for-hyperparameter-tuning-c2468a2ff887\n",
    "\n",
    "https://trainindata.medium.com/recursive-feature-elimination-with-python-59bb27e8396a\n",
    "\n",
    "https://towardsdatascience.com/how-to-tune-multiple-ml-models-with-gridsearchcv-at-once-9fcebfcc6c23\n",
    "\n",
    "https://towardsdatascience.com/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8\n",
    "\n",
    "https://medium.com/@okanyenigun/cross-validation-techniques-for-machine-learning-a-guide-to-improve-model-performance-8748d46281cc\n",
    "\n",
    "https://towardsdatascience.com/why-you-should-use-scikit-learn-pipelines-8754b4d1e375\n",
    "\n",
    "https://towardsdatascience.com/benchmarking-machine-learning-models-with-cross-validation-and-matplotlib-in-python-4957a41149e\n",
    "\n",
    "https://towardsdatascience.com/k-fold-cross-validation-are-you-doing-it-right-e98cdf3e6690\n",
    "\n",
    "https://towardsdatascience.com/complete-guide-to-regressional-analysis-using-python-bbe76b3e451f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust the path based on the location of your notebook\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "# Second cell: Import the PATH from settings\n",
    "from utils.settings import PATH\n",
    "\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages to develop the model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "# import sklearn.preprocessing as preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge: L2 penalty.\n",
    "\n",
    "Lasso: L1 penalty.\n",
    "\n",
    "Elastic-Net: Combination of L1 and L2 penalty.\n",
    "\n",
    "It is important to scale the data (e.g., using a StandardScaler) before performing ridge\n",
    "regression, as it is sensitive to the scale of the input features. This is true of most\n",
    "regularized models.\n",
    "\n",
    "The RidgeCV class also performs ridge regression, but it automatically tunes\n",
    "hyperparameters using cross-validation. It’s roughly equivalent to using GridSearchCV,\n",
    "but it’s optimized for ridge regression and runs much faster. Several other estimators\n",
    "(mostly linear) also have efficient CV variants, such as LassoCV and ElasticNetCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None, max_iter=1000, eta0=0.01, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel()) # y.ravel() because fit() expects 1D targets\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5) # When r = 0 elastic net is equivalent to ridge regression, and when r = 1, it is equivalent to lasso regression\n",
    "elastic_net.fit(X, y)\n",
    "\n",
    "LogisticRegression(C=1.0, penalty='l2',tol=0.01)\n",
    "\n",
    "ridge = Ridge(normalize=True)\n",
    "second_order=PolynomialFeatures(degree=2, interaction_only=False)\n",
    "ridge.fit(second_order.fit_transform(X), y)\n",
    "lm.fit(second_order.fit_transform(X), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "edges = np.histogram(y, bins=5)[1]\n",
    "binning = np.digitize(y, edges)\n",
    "stratified_cv_iterator = StratifiedKFold(binning, n_folds=10,shuffle=True, random_state=101)\n",
    "search = GridSearchCV(param_grid={'alpha':np.logspace(-4,2,7)},\n",
    "estimator=ridge, scoring ='mean_squared_error',\n",
    "n_jobs=1, refit=True, cv=stratified_cv_iterator)\n",
    "search.fit(second_order.fit_transform(X), y)\n",
    "print ('Best alpha: %0.5f' % search.best_params_['alpha'])\n",
    "print ('Best CV mean squared error: %0.3f' % np.abs(search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1.0, normalize=True, max_iter=10**5)\n",
    "#The following comment shows an example of L1 logistic regression\n",
    "#lr_l1 = LogisticRegression(C=1.0, penalty='l1', tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from scipy.stats import expon\n",
    "np.random.seed(101)\n",
    "search_func=RandomizedSearchCV(estimator=lasso, n_jobs=1, iid=False, refit=True, n_iter=15,\n",
    "param_distributions={'alpha':np.logspace(-5,2,100)},\n",
    "scoring='mean_squared_error', cv=stratified_cv_iterator)\n",
    "search_func.fit(second_order.fit_transform(X), y)\n",
    "print ('Best alpha: %0.5f' % search_func.best_params_['alpha'])\n",
    "print ('Best CV mean squared error: %0.3f' % np.abs(search_func.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elasticnet = ElasticNet(alpha=1.0, l1_ratio=0.15, normalize=True, max_iter=10**6, random_state=101)\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from scipy.stats import expon\n",
    "np.random.seed(101)\n",
    "search_func=RandomizedSearchCV(estimator=elasticnet, param_distributions={'alpha':np.logspace(-5,2,100), 'l1_ratio':np.arange(0.0, 1.01, 0.05)}, n_iter=30, scoring='mean_squared_error', n_jobs=1, iid=False, refit=True, cv=stratified_cv_iterator)\n",
    "search_func.fit(second_order.fit_transform(X), y)\n",
    "print ('Best alpha: %0.5f' % search_func.best_params_['alpha'])\n",
    "print ('Best l1_ratio: %0.5f' % search_func.best_params_['l1_ratio'])\n",
    "print ('Best CV mean squared error: %0.3f' % \\\n",
    "np.abs(search_func.best_score_))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
